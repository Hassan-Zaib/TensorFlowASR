{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"speech_config\": {\n",
    "        \"sample_rate\": 16000,\n",
    "        \"frame_ms\": 25,\n",
    "        \"stride_ms\": 10,\n",
    "        \"num_feature_bins\": 80,\n",
    "        \"feature_type\": \"log_mel_spectrogram\",\n",
    "        \"preemphasis\": 0.97,\n",
    "        \"normalize_signal\": True,\n",
    "        \"normalize_feature\": True,\n",
    "        \"normalize_per_frame\": False,\n",
    "    },\n",
    "    \"decoder_config\": {\n",
    "        \"vocabulary\": None,\n",
    "        \"target_vocab_size\": 1024,\n",
    "        \"max_subword_length\": 4,\n",
    "        \"blank_at_zero\": True,\n",
    "        \"beam_width\": 5,\n",
    "        \"norm_score\": True,\n",
    "    },\n",
    "    \"model_config\": {\n",
    "        \"name\": \"contextnet\",\n",
    "        \"encoder_alpha\": 0.5,\n",
    "        \"encoder_blocks\": [\n",
    "            {\n",
    "                \"nlayers\": 1,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": False,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 2,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 2,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 256,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 2,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 5,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 512,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": True,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "            {\n",
    "                \"nlayers\": 1,\n",
    "                \"kernel_size\": 5,\n",
    "                \"filters\": 640,\n",
    "                \"strides\": 1,\n",
    "                \"residual\": False,\n",
    "                \"activation\": \"silu\",\n",
    "            },\n",
    "        ],\n",
    "        \"prediction_embed_dim\": 640,\n",
    "        \"prediction_embed_dropout\": 0,\n",
    "        \"prediction_num_rnns\": 1,\n",
    "        \"prediction_rnn_units\": 640,\n",
    "        \"prediction_rnn_type\": \"lstm\",\n",
    "        \"prediction_rnn_implementation\": 1,\n",
    "        \"prediction_layer_norm\": True,\n",
    "        \"prediction_projection_units\": 0,\n",
    "        \"joint_dim\": 640,\n",
    "        \"joint_activation\": \"tanh\",\n",
    "    },\n",
    "    \"learning_config\": {\n",
    "        \"train_dataset_config\": {\n",
    "            \"use_tf\": True,\n",
    "            \"augmentation_config\": {\n",
    "                \"feature_augment\": {\n",
    "                    \"time_masking\": {\n",
    "                        \"num_masks\": 10,\n",
    "                        \"mask_factor\": 100,\n",
    "                        \"p_upperbound\": 0.05,\n",
    "                    },\n",
    "                    \"freq_masking\": {\"num_masks\": 1, \"mask_factor\": 27},\n",
    "                }\n",
    "            },\n",
    "            \"data_paths\": [\n",
    "                \"/mnt/h/ML/Datasets/ASR/Raw/LibriSpeech/train-clean-100/transcripts.tsv\"\n",
    "            ],\n",
    "            \"tfrecords_dir\": None,\n",
    "            \"shuffle\": True,\n",
    "            \"cache\": True,\n",
    "            \"buffer_size\": 100,\n",
    "            \"drop_remainder\": True,\n",
    "            \"stage\": \"train\",\n",
    "        },\n",
    "        \"eval_dataset_config\": {\n",
    "            \"use_tf\": True,\n",
    "            \"data_paths\": None,\n",
    "            \"tfrecords_dir\": None,\n",
    "            \"shuffle\": False,\n",
    "            \"cache\": True,\n",
    "            \"buffer_size\": 100,\n",
    "            \"drop_remainder\": True,\n",
    "            \"stage\": \"eval\",\n",
    "        },\n",
    "        \"test_dataset_config\": {\n",
    "            \"use_tf\": True,\n",
    "            \"data_paths\": [\n",
    "                \"/mnt/h/ML/Datasets/ASR/Raw/LibriSpeech/test-clean/transcripts.tsv\"\n",
    "            ],\n",
    "            \"tfrecords_dir\": None,\n",
    "            \"shuffle\": False,\n",
    "            \"cache\": True,\n",
    "            \"buffer_size\": 100,\n",
    "            \"drop_remainder\": True,\n",
    "            \"stage\": \"test\",\n",
    "        },\n",
    "        \"optimizer_config\": {\n",
    "            \"warmup_steps\": 40000,\n",
    "            \"beta_1\": 0.9,\n",
    "            \"beta_2\": 0.98,\n",
    "            \"epsilon\": 1e-09,\n",
    "        },\n",
    "        \"running_config\": {\n",
    "            \"batch_size\": 2,\n",
    "            \"num_epochs\": 20,\n",
    "            \"checkpoint\": {\n",
    "                \"filepath\": \"/mnt/e/Models/local/contextnet/checkpoints/{epoch:02d}.h5\",\n",
    "                \"save_best_only\": False,\n",
    "                \"save_weights_only\": True,\n",
    "                \"save_freq\": \"epoch\",\n",
    "            },\n",
    "            \"states_dir\": \"/mnt/e/Models/local/contextnet/states\",\n",
    "            \"tensorboard\": {\n",
    "                \"log_dir\": \"/mnt/e/Models/local/contextnet/tensorboard\",\n",
    "                \"histogram_freq\": 1,\n",
    "                \"write_graph\": True,\n",
    "                \"write_images\": True,\n",
    "                \"update_freq\": \"epoch\",\n",
    "                \"profile_batch\": 2,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mydata/hassan/TensorFlowASR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"train\": {\"max_input_length\": 2974, \"max_label_length\": 194, \"num_entries\": 281241},\n",
    "    \"eval\": {\"max_input_length\": 3516, \"max_label_length\": 186, \"num_entries\": 5567},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 09:46:06.876000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-17 09:46:06.876023: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please provide a TPU Name to connect to.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 09:46:08.136288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-17 09:46:08.136310: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-17 09:46:08.136342: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node0.tempnode.dsdm-pg0.clemson.cloudlab.us): /proc/driver/nvidia/version does not exist\n",
      "2023-05-17 09:46:08.136585: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use RNNT loss in TensorFlow\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "/mnt/h; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_asr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransducer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontextnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextNet\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_asr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerSchedule\n\u001b[0;32m---> 19\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m speech_featurizer \u001b[38;5;241m=\u001b[39m speech_featurizers\u001b[38;5;241m.\u001b[39mTFSpeechFeaturizer(config\u001b[38;5;241m.\u001b[39mspeech_config)\n\u001b[1;32m     22\u001b[0m text_featurizer \u001b[38;5;241m=\u001b[39m text_featurizers\u001b[38;5;241m.\u001b[39mCharFeaturizer(config\u001b[38;5;241m.\u001b[39mdecoder_config)\n",
      "File \u001b[0;32m/mydata/hassan/TensorFlowASR/tensorflow_asr/configs/config.py:101\u001b[0m, in \u001b[0;36mConfig.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_config \u001b[38;5;241m=\u001b[39m \u001b[43mLearningConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, v)\n",
      "File \u001b[0;32m/mydata/hassan/TensorFlowASR/tensorflow_asr/configs/config.py:84\u001b[0m, in \u001b[0;36mLearningConfig.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config:\n\u001b[1;32m     83\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset_config \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_dataset_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset_config \u001b[38;5;241m=\u001b[39m DatasetConfig(config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_dataset_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset_config \u001b[38;5;241m=\u001b[39m DatasetConfig(config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_dataset_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n",
      "File \u001b[0;32m/mydata/hassan/TensorFlowASR/tensorflow_asr/configs/config.py:48\u001b[0m, in \u001b[0;36mDatasetConfig.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     46\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_paths \u001b[38;5;241m=\u001b[39m \u001b[43mfile_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_paths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecords_dir \u001b[38;5;241m=\u001b[39m file_util\u001b[38;5;241m.\u001b[39mpreprocess_paths(config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfrecords_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), isdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecords_shards \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfrecords_shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[0;32m/mydata/hassan/TensorFlowASR/tensorflow_asr/utils/file_util.py:92\u001b[0m, in \u001b[0;36mpreprocess_paths\u001b[0;34m(paths, isdir)\u001b[0m\n\u001b[1;32m     90\u001b[0m         dirpath \u001b[38;5;241m=\u001b[39m path \u001b[38;5;28;01mif\u001b[39;00m isdir \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path)\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(dirpath):\n\u001b[0;32m---> 92\u001b[0m             \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m paths\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(paths, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/mydata/anaconda3/envs/tensorflow_25/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py:511\u001b[0m, in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.gfile.makedirs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_create_dir_v2\u001b[39m(path):\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m  It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursivelyCreateDir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /mnt/h; Permission denied"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from tensorflow_asr.utils import env_util\n",
    "\n",
    "env_util.setup_environment()\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "strategy = env_util.setup_strategy([0])\n",
    "\n",
    "from tensorflow_asr.configs.config import Config\n",
    "from tensorflow_asr.datasets import asr_dataset\n",
    "from tensorflow_asr.featurizers import speech_featurizers, text_featurizers\n",
    "from tensorflow_asr.models.transducer.contextnet import ContextNet\n",
    "from tensorflow_asr.optimizers.schedules import TransformerSchedule\n",
    "\n",
    "config = Config(config)\n",
    "speech_featurizer = speech_featurizers.TFSpeechFeaturizer(config.speech_config)\n",
    "\n",
    "text_featurizer = text_featurizers.CharFeaturizer(config.decoder_config)\n",
    "\n",
    "train_dataset = asr_dataset.ASRSliceDataset(\n",
    "    speech_featurizer=speech_featurizer,\n",
    "    text_featurizer=text_featurizer,\n",
    "    **vars(config.learning_config.train_dataset_config),\n",
    "    indefinite=True\n",
    ")\n",
    "eval_dataset = asr_dataset.ASRSliceDataset(\n",
    "    speech_featurizer=speech_featurizer,\n",
    "    text_featurizer=text_featurizer,\n",
    "    **vars(config.learning_config.eval_dataset_config),\n",
    "    indefinite=True\n",
    ")\n",
    "\n",
    "train_dataset.load_metadata(metadata)\n",
    "eval_dataset.load_metadata(metadata)\n",
    "speech_featurizer.reset_length()\n",
    "text_featurizer.reset_length()\n",
    "\n",
    "global_batch_size = config.learning_config.running_config.batch_size\n",
    "global_batch_size *= strategy.num_replicas_in_sync\n",
    "\n",
    "train_data_loader = train_dataset.create(global_batch_size)\n",
    "eval_data_loader = eval_dataset.create(global_batch_size)\n",
    "\n",
    "with strategy.scope():\n",
    "    # build model\n",
    "    contextnet = ContextNet(**config.model_config, vocabulary_size=text_featurizer.num_classes)\n",
    "    contextnet.make(speech_featurizer.shape)\n",
    "    contextnet.summary(line_length=100)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        TransformerSchedule(\n",
    "            d_model=contextnet.dmodel,\n",
    "            warmup_steps=config.learning_config.optimizer_config.pop(\"warmup_steps\", 10000),\n",
    "            max_lr=(0.05 / math.sqrt(contextnet.dmodel))\n",
    "        ),\n",
    "        **config.learning_config.optimizer_config\n",
    "    )\n",
    "\n",
    "    contextnet.compile(\n",
    "        optimizer=optimizer,\n",
    "        experimental_steps_per_execution=10,\n",
    "        global_batch_size=global_batch_size,\n",
    "        blank=text_featurizer.blank\n",
    "    )\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(**config.learning_config.running_config.checkpoint),\n",
    "    tf.keras.callbacks.experimental.BackupAndRestore(config.learning_config.running_config.states_dir),\n",
    "    tf.keras.callbacks.TensorBoard(**config.learning_config.running_config.tensorboard)\n",
    "]\n",
    "\n",
    "contextnet.fit(\n",
    "    train_data_loader,\n",
    "    epochs=config.learning_config.running_config.num_epochs,\n",
    "    validation_data=eval_data_loader,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=train_dataset.total_steps,\n",
    "    validation_steps=eval_dataset.total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "metadata": {
   "interpreter": {
    "hash": "45f983f364f7a4cc7101e6d6987a2125bf0c2b5c5c9855ff35103689f542d13f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
